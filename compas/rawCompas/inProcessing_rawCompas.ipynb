{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, recall_score, f1_score\n",
    "from aif360.datasets import BinaryLabelDataset\n",
    "from aif360.metrics import ClassificationMetric\n",
    "from sklearn.model_selection import train_test_split\n",
    "from aif360.sklearn.inprocessing import ExponentiatedGradientReduction\n",
    "from sklearn.svm import LinearSVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_performance_metrics(y_test, y_pred, model_name):\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    recall = recall_score(y_test, y_pred)\n",
    "    f1_score_value = f1_score(y_test, y_pred)\n",
    "    print(f\"{model_name} Accuracy: {accuracy}\")\n",
    "    print(f\"{model_name} Recall: {recall}\")\n",
    "    print(f\"{model_name} F1 Score: {f1_score_value}\")\n",
    "\n",
    "def compute_classification_metric(dataset,predictions, label_name_v, favorable_label_v, unfavorable_label_v, privileged_attribute, unprivileged_attribute):\n",
    "    features = [privileged_attribute] + unprivileged_attribute # We want to check the fairness level regarding the protected attribute \"sex\"\n",
    "\n",
    "    # This is the object made of the original dataset\n",
    "    aif_sex_dataset = BinaryLabelDataset( # Base class for all structured datasets with binary labels.\n",
    "            df=dataset,\n",
    "            favorable_label=favorable_label_v, # This means that a prediction is biased toward the privileged attribute if its value is 1 (True)\n",
    "            unfavorable_label=unfavorable_label_v,\n",
    "            label_names=[label_name_v],\n",
    "            protected_attribute_names=features,\n",
    "            privileged_protected_attributes=[privileged_attribute], # here we tell AIF that we want to check for predictions\n",
    "        )\n",
    "\n",
    "    # We do the same thing but with the predictions dataset\n",
    "    aif_sex_pred = BinaryLabelDataset(\n",
    "            df=predictions,\n",
    "            favorable_label=favorable_label_v,\n",
    "            unfavorable_label=unfavorable_label_v,\n",
    "            label_names=[label_name_v],\n",
    "            protected_attribute_names=features,\n",
    "            privileged_protected_attributes=[privileged_attribute],\n",
    "        )\n",
    "\n",
    "    sex_privileged_group = [{privileged_attribute: 1, **{attr: 0 for attr in unprivileged_attribute}}]\n",
    "    sex_unprivileged_group = [{attr: 1, privileged_attribute: 0} for attr in unprivileged_attribute]\n",
    "\n",
    "    # We provide the ClassificationMetric object with all the information needed:\n",
    "    # aif_sex_dataset - The original test set\n",
    "    # aif_sex_pred - A dataset containing the predictions of the model\n",
    "    # sex_privileged_group - The privileged group\n",
    "    # sex_unprivileged_group - The unprivileged group\n",
    "    fairness_metrics = ClassificationMetric(dataset=aif_sex_dataset,\n",
    "                            classified_dataset=aif_sex_pred,\n",
    "                            unprivileged_groups=sex_unprivileged_group,\n",
    "                            privileged_groups=sex_privileged_group)\n",
    "    \n",
    "    return fairness_metrics\n",
    "\n",
    "def compute_fairness_metrics(fairness_metrics: ClassificationMetric):\n",
    "    # Values less than 0 indicate that privileged group has higher\n",
    "    # proportion of predicted positive outcomes than unprivileged group.\n",
    "    # Value higher than 0 indicates that unprivileged group has higher proportion\n",
    "    # of predicted positive outcomes than privileged group.\n",
    "    SPD = round(fairness_metrics.statistical_parity_difference(),3)\n",
    "\n",
    "    # Measures the deviation from the equality of opportunity, which means that the same\n",
    "    # proportion of each population receives the favorable outcome. This measure must be equal to 0 to be fair.\n",
    "    EOD = round(fairness_metrics.equal_opportunity_difference(),3)\n",
    "\n",
    "    # Average of difference in False Positive Rate and True Positive Rate for unprivileged and privileged groups\n",
    "    # A value of 0 indicates equality of odds, which means that samples in both the privileged and unprivileged\n",
    "    # groups have the same probability of being classified positively.\n",
    "    AOD = round(fairness_metrics.average_odds_difference(),3)\n",
    "\n",
    "    print(f\"Statistical Parity Difference (SPD): {SPD}\")\n",
    "    print(f\"Average Odds Difference (AOD): {AOD}\")\n",
    "    print(f\"Equal Opportunity Difference (EOD): {EOD}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = \"trained_compas-score.csv\"\n",
    "df_raw = pd.read_csv(dataset_path) \n",
    "\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_raw.drop(columns=\"is_recid\")\n",
    "y = df_raw[\"is_recid\"]\n",
    "\n",
    "# Define four sets and apply the function\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y,\n",
    "                                                    test_size=0.2, # 0.2 indicates a test set size of 20%\n",
    "                                                    random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "sex_features = [\"sex_Male\", \"sex_Female\"]\n",
    "race_features = [\"race_Caucasian\", \"race_African-American\", \"race_Asian\", \"race_Other\", \"race_Native American\", \"race_Hispanic\"]\n",
    "\n",
    "race_privileged = \"race_Caucasian\"\n",
    "race_unprivileged = [\"race_African-American\", \"race_Asian\", \"race_Other\", \"race_Native American\", \"race_Hispanic\"]\n",
    "sex_privileged = \"sex_Female\"\n",
    "sex_unprivileged = [\"sex_Male\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, we train our decision tree regularly\n",
    "dt_clf = DecisionTreeClassifier(random_state=42)\n",
    "\n",
    "# We create an ExponentiatedGradientReduction object that acts on the trained model to change its internal behavior\n",
    "processed_dt_clf = ExponentiatedGradientReduction(prot_attr=sex_features, estimator=dt_clf, constraints='DemographicParity', drop_prot_attr=False)\n",
    "\n",
    "# Then we train again the same model updated with the reduction\n",
    "fair_dt_clf = processed_dt_clf.fit(X_train, y_train)\n",
    "\n",
    "# After the training phase, the model will be tested by predicting the values on the test set\n",
    "fair_dt_predictions = fair_dt_clf.predict(X_test)\n",
    "\n",
    "compute_performance_metrics(y_test,fair_dt_predictions,\"Decision Tree\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definisci la pipeline e dai un nome al classificatore\n",
    "svm_pipeline = make_pipeline(StandardScaler(), LinearSVC(random_state=0, tol=1e-5))\n",
    "\n",
    "svm_classifier = svm_pipeline.named_steps['linearsvc'].fit(X_train, y_train.values.ravel())\n",
    "\n",
    "# Crea l'oggetto ExponentiatedGradientReduction\n",
    "processed_svm = ExponentiatedGradientReduction(prot_attr=sex_features, estimator=svm_classifier, constraints='DemographicParity', drop_prot_attr=False)\n",
    "\n",
    "# Fai l'addestramento passando sample_weight al classificatore nella pipeline\n",
    "fair_svm = processed_svm.fit(X_train, y_train)\n",
    "\n",
    "# Fai delle predizioni sui dati di test\n",
    "svm_pred = fair_svm.predict(X_test)\n",
    "\n",
    "# Calcola le metriche di performance\n",
    "compute_performance_metrics(y_test, svm_pred, \"SVM\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_classifier = RandomForestClassifier(n_estimators=100, criterion='gini', max_depth = None, random_state=42)\n",
    "\n",
    "#rf_classifier.fit(X_train, y_train)\n",
    "\n",
    "processed_rf = ExponentiatedGradientReduction(prot_attr=sex_features, estimator=svm_classifier, constraints='DemographicParity', drop_prot_attr=False)\n",
    "\n",
    "fair_rf = processed_rf.fit(X_train, y_train)\n",
    "\n",
    "rf_predictions = fair_rf.predict(X_test)\n",
    "\n",
    "compute_performance_metrics(y_test,rf_predictions,\"Random Forest\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CALCOLO LE METRICHE DI FAIRNESS CONSIDERANDO COME MODELLO IL DECISION TREE\n",
    "dataset = X_test.copy(deep=True) # we create a copy of the test set\n",
    "dataset['is_recid'] = y_test  # and join the target feature with the others\n",
    "predictions = dataset.copy(deep=True) # we do the same task\n",
    "predictions['is_recid'] = fair_dt_predictions # but this time the target feature is made by the predictions of our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Statistical Parity Difference (SPD): 0.024\n",
      "Average Odds Difference (AOD): 0.016\n",
      "Equal Opportunity Difference (EOD): 0.066\n"
     ]
    }
   ],
   "source": [
    "fairness_metrics = compute_classification_metric(dataset,predictions,\"is_recid\",0,1,sex_privileged,sex_unprivileged)\n",
    "compute_fairness_metrics(fairness_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Statistical Parity Difference (SPD): -0.016\n",
      "Average Odds Difference (AOD): -0.011\n",
      "Equal Opportunity Difference (EOD): 0.027\n"
     ]
    }
   ],
   "source": [
    "predictions['is_recid'] = svm_pred \n",
    "\n",
    "fairness_metrics = compute_classification_metric(dataset,predictions,\"is_recid\",0,1,sex_privileged,sex_unprivileged)\n",
    "compute_fairness_metrics(fairness_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Statistical Parity Difference (SPD): -0.022\n",
      "Average Odds Difference (AOD): -0.011\n",
      "Equal Opportunity Difference (EOD): 0.012\n"
     ]
    }
   ],
   "source": [
    "predictions['is_recid'] = rf_predictions \n",
    "\n",
    "fairness_metrics = compute_classification_metric(dataset,predictions,\"is_recid\",0,1,sex_privileged,sex_unprivileged)\n",
    "compute_fairness_metrics(fairness_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, we train our decision tree regularly\n",
    "dt_clf = DecisionTreeClassifier(random_state=42)\n",
    "\n",
    "# We create an ExponentiatedGradientReduction object that acts on the trained model to change its internal behavior\n",
    "processed_dt_clf = ExponentiatedGradientReduction(prot_attr=race_features, estimator=dt_clf, constraints='DemographicParity', drop_prot_attr=False)\n",
    "\n",
    "# Then we train again the same model updated with the reduction\n",
    "fair_dt_clf = processed_dt_clf.fit(X_train, y_train)\n",
    "\n",
    "# After the training phase, the model will be tested by predicting the values on the test set\n",
    "fair_dt_predictions = fair_dt_clf.predict(X_test)\n",
    "\n",
    "compute_performance_metrics(y_test,fair_dt_predictions,\"Decision Tree\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definisci la pipeline e dai un nome al classificatore\n",
    "svm_pipeline = make_pipeline(StandardScaler(), LinearSVC(random_state=0, tol=1e-5))\n",
    "\n",
    "svm_classifier = svm_pipeline.named_steps['linearsvc'].fit(X_train, y_train.values.ravel())\n",
    "\n",
    "# Crea l'oggetto ExponentiatedGradientReduction\n",
    "processed_svm = ExponentiatedGradientReduction(prot_attr=race_features, estimator=svm_classifier, constraints='DemographicParity', drop_prot_attr=False)\n",
    "\n",
    "# Fai l'addestramento passando sample_weight al classificatore nella pipeline\n",
    "fair_svm = processed_svm.fit(X_train, y_train)\n",
    "\n",
    "# Fai delle predizioni sui dati di test\n",
    "svm_pred = fair_svm.predict(X_test)\n",
    "\n",
    "# Calcola le metriche di performance\n",
    "compute_performance_metrics(y_test, svm_pred, \"SVM\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_classifier = RandomForestClassifier(n_estimators=100, criterion='gini', max_depth = None, random_state=42)\n",
    "\n",
    "#rf_classifier.fit(X_train, y_train)\n",
    "\n",
    "processed_rf = ExponentiatedGradientReduction(prot_attr=race_features, estimator=svm_classifier, constraints='DemographicParity', drop_prot_attr=False)\n",
    "\n",
    "fair_rf = processed_rf.fit(X_train, y_train)\n",
    "\n",
    "rf_predictions = fair_rf.predict(X_test)\n",
    "\n",
    "compute_performance_metrics(y_test,rf_predictions,\"Random Forest\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CALCOLO LE METRICHE DI FAIRNESS CONSIDERANDO COME MODELLO IL DECISION TREE\n",
    "dataset = X_test.copy(deep=True) # we create a copy of the test set\n",
    "dataset['is_recid'] = y_test  # and join the target feature with the others\n",
    "predictions = dataset.copy(deep=True) # we do the same task\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Statistical Parity Difference (SPD): -0.02\n",
      "Average Odds Difference (AOD): -0.011\n",
      "Equal Opportunity Difference (EOD): 0.0\n"
     ]
    }
   ],
   "source": [
    "predictions['is_recid'] = fair_dt_predictions # but this time the target feature is made by the predictions of our model\n",
    "\n",
    "fairness_metrics = compute_classification_metric(dataset,predictions,\"is_recid\",0,1,race_privileged,race_unprivileged)\n",
    "compute_fairness_metrics(fairness_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Statistical Parity Difference (SPD): -0.031\n",
      "Average Odds Difference (AOD): 0.001\n",
      "Equal Opportunity Difference (EOD): -0.026\n"
     ]
    }
   ],
   "source": [
    "predictions['is_recid'] = svm_pred \n",
    "\n",
    "fairness_metrics = compute_classification_metric(dataset,predictions,\"is_recid\",0,1,race_privileged,race_unprivileged)\n",
    "compute_fairness_metrics(fairness_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Statistical Parity Difference (SPD): -0.028\n",
      "Average Odds Difference (AOD): 0.003\n",
      "Equal Opportunity Difference (EOD): -0.019\n"
     ]
    }
   ],
   "source": [
    "predictions['is_recid'] = rf_predictions \n",
    "\n",
    "fairness_metrics = compute_classification_metric(dataset,predictions,\"is_recid\",0,1,race_privileged,race_unprivileged)\n",
    "compute_fairness_metrics(fairness_metrics)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "se4faiEnv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
