{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\mario\\Desktop\\se4ai\\Progetto\\Fairness_datasmell_SE4AI\\env\\lib\\site-packages\\torch\\_functorch\\deprecated.py:61: UserWarning: We've integrated functorch into PyTorch. As the final step of the integration, functorch.vmap is deprecated as of PyTorch 2.0 and will be deleted in a future version of PyTorch >= 2.3. Please use torch.vmap instead; see the PyTorch 2.0 release notes and/or the torch.func migration guide for more details https://pytorch.org/docs/master/func.migrating.html\n",
      "  warn_deprecated('vmap', 'torch.vmap')\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, recall_score, f1_score\n",
    "from aif360.datasets import BinaryLabelDataset\n",
    "from aif360.metrics import ClassificationMetric\n",
    "from sklearn.model_selection import train_test_split\n",
    "from aif360.algorithms.postprocessing import EqOddsPostprocessing\n",
    "from sklearn.pipeline import Pipeline\n",
    "from aif360.algorithms.postprocessing import DeterministicReranking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_performance_metrics(y_test, y_pred, model_name):\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    recall = recall_score(y_test, y_pred)\n",
    "    f1_score_value = f1_score(y_test, y_pred)\n",
    "    print(f\"{model_name} Accuracy: {accuracy}\")\n",
    "    print(f\"{model_name} Recall: {recall}\")\n",
    "    print(f\"{model_name} F1 Score: {f1_score_value}\")\n",
    "\n",
    "def compute_classification_metric(dataset,predictions, label_name_v, favorable_label_v, unfavorable_label_v, privileged_attribute, unprivileged_attribute):\n",
    "    features = [privileged_attribute] + unprivileged_attribute # We want to check the fairness level regarding the protected attribute \"sex\"\n",
    "\n",
    "    # This is the object made of the original dataset\n",
    "    aif_sex_dataset = BinaryLabelDataset( # Base class for all structured datasets with binary labels.\n",
    "            df=dataset,\n",
    "            favorable_label=favorable_label_v, # This means that a prediction is biased toward the privileged attribute if its value is 1 (True)\n",
    "            unfavorable_label=unfavorable_label_v,\n",
    "            label_names=[label_name_v],\n",
    "            protected_attribute_names=features,\n",
    "            privileged_protected_attributes=[privileged_attribute], # here we tell AIF that we want to check for predictions\n",
    "        )\n",
    "\n",
    "    # We do the same thing but with the predictions dataset\n",
    "    aif_sex_pred = BinaryLabelDataset(\n",
    "            df=predictions,\n",
    "            favorable_label=favorable_label_v,\n",
    "            unfavorable_label=unfavorable_label_v,\n",
    "            label_names=[label_name_v],\n",
    "            protected_attribute_names=features,\n",
    "            privileged_protected_attributes=[privileged_attribute],\n",
    "        )\n",
    "\n",
    "    sex_privileged_group = [{privileged_attribute: 1, **{attr: 0 for attr in unprivileged_attribute}}]\n",
    "    sex_unprivileged_group = [{attr: 1, privileged_attribute: 0} for attr in unprivileged_attribute]\n",
    "\n",
    "    # We provide the ClassificationMetric object with all the information needed:\n",
    "    # aif_sex_dataset - The original test set\n",
    "    # aif_sex_pred - A dataset containing the predictions of the model\n",
    "    # sex_privileged_group - The privileged group\n",
    "    # sex_unprivileged_group - The unprivileged group\n",
    "    fairness_metrics = ClassificationMetric(dataset=aif_sex_dataset,\n",
    "                            classified_dataset=aif_sex_pred,\n",
    "                            unprivileged_groups=sex_unprivileged_group,\n",
    "                            privileged_groups=sex_privileged_group)\n",
    "    \n",
    "    return fairness_metrics\n",
    "\n",
    "def compute_fairness_metrics(fairness_metrics: ClassificationMetric):\n",
    "    # Values less than 0 indicate that privileged group has higher\n",
    "    # proportion of predicted positive outcomes than unprivileged group.\n",
    "    # Value higher than 0 indicates that unprivileged group has higher proportion\n",
    "    # of predicted positive outcomes than privileged group.\n",
    "    SPD = round(fairness_metrics.statistical_parity_difference(),3)\n",
    "\n",
    "    # Measures the deviation from the equality of opportunity, which means that the same\n",
    "    # proportion of each population receives the favorable outcome. This measure must be equal to 0 to be fair.\n",
    "    EOD = round(fairness_metrics.equal_opportunity_difference(),3)\n",
    "\n",
    "    # Average of difference in False Positive Rate and True Positive Rate for unprivileged and privileged groups\n",
    "    # A value of 0 indicates equality of odds, which means that samples in both the privileged and unprivileged\n",
    "    # groups have the same probability of being classified positively.\n",
    "    AOD = round(fairness_metrics.average_odds_difference(),3)\n",
    "\n",
    "    print(f\"Statistical Parity Difference (SPD): {SPD}\")\n",
    "    print(f\"Average Odds Difference (AOD): {AOD}\")\n",
    "    print(f\"Equal Opportunity Difference (EOD): {EOD}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = \"trained_tidy_compas.csv\"\n",
    "df_raw = pd.read_csv(dataset_path) \n",
    "\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_raw.drop(columns=\"is_recid\")\n",
    "y = df_raw[\"is_recid\"]\n",
    "\n",
    "# Define four sets and apply the function\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y,\n",
    "                                                    test_size=0.2, # 0.2 indicates a test set size of 20%\n",
    "                                                    random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "sex_features = [\"sex_Male\", \"sex_Female\"]\n",
    "race_features = [\"race_Caucasian\", \"race_African-American\", \"race_Asian\", \"race_Other\", \"race_Native American\", \"race_Hispanic\"]\n",
    "\n",
    "race_privileged = \"race_Caucasian\"\n",
    "race_unprivileged = [\"race_African-American\", \"race_Asian\", \"race_Other\", \"race_Native American\", \"race_Hispanic\"]\n",
    "sex_privileged = \"sex_Female\"\n",
    "sex_unprivileged = [\"sex_Male\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree Accuracy: 0.6578778135048231\n",
      "Decision Tree Recall: 0.3595505617977528\n",
      "Decision Tree F1 Score: 0.3755868544600939\n",
      "Decision Tree Post-processed Accuracy: 0.6482315112540193\n",
      "Decision Tree Post-processed Recall: 0.298876404494382\n",
      "Decision Tree Post-processed F1 Score: 0.32718327183271834\n"
     ]
    }
   ],
   "source": [
    "# Train the Decision Tree classifier\n",
    "dt_clf = DecisionTreeClassifier(random_state=42)\n",
    "dt_clf.fit(X_train, y_train)\n",
    "\n",
    "# Predizione sui dati di test\n",
    "dt_predictions = dt_clf.predict(X_test)\n",
    "\n",
    "# Calcolo delle metriche di performance per il Decision Tree grezzo\n",
    "compute_performance_metrics(y_test, dt_predictions, \"Decision Tree\")\n",
    "\n",
    "# Aggiunta della colonna del target al dataframe di X_test e rimozione di valori mancanti\n",
    "dataset = X_test.copy(deep=True)\n",
    "dataset['is_recid'] = y_test\n",
    "dataset = dataset.dropna()\n",
    "\n",
    "# Conversione dei dati in formato BinaryLabelDataset\n",
    "test_dataset = BinaryLabelDataset(df=dataset, label_names=['is_recid'], protected_attribute_names=sex_features)\n",
    "\n",
    "# Creazione di un dataset di predizioni per il post-processing\n",
    "predictions = dataset.copy(deep=True)\n",
    "predictions['is_recid'] = dt_predictions\n",
    "predictions = predictions.dropna()\n",
    "\n",
    "test_pred_dataset = BinaryLabelDataset(df=predictions, label_names=['is_recid'], protected_attribute_names=sex_features)\n",
    "\n",
    "# Applicazione del post-processing con EqOddsPostprocessing\n",
    "eq_odds = EqOddsPostprocessing(privileged_groups=[{'sex_Female': 1}], unprivileged_groups=[{'sex_Female': 0}])\n",
    "eq_odds = eq_odds.fit(test_dataset, test_pred_dataset)\n",
    "\n",
    "# predizioni post-processate\n",
    "eq_odds_pred = eq_odds.predict(test_pred_dataset)\n",
    "eq_odds_labels = eq_odds_pred.labels\n",
    "\n",
    "# Calcolo delle metriche di performance per il modello post-processato\n",
    "compute_performance_metrics(y_test, eq_odds_labels, \"Decision Tree Post-processed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creazione del dataset di test con le etichette originali e predizioni del modello\n",
    "test_dataset_with_labels = X_test.copy(deep=True)\n",
    "test_dataset_with_labels['is_recid'] = y_test\n",
    "\n",
    "# Creazione del dataset di predizioni del modello post-processato\n",
    "predictions_post_processed = X_test.copy(deep=True)\n",
    "predictions_post_processed['is_recid'] = eq_odds_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Statistical Parity Difference (SPD): -0.01\n",
      "Average Odds Difference (AOD): -0.0\n",
      "Equal Opportunity Difference (EOD): 0.0\n"
     ]
    }
   ],
   "source": [
    "# Calcolo delle metriche di fairness\n",
    "fairness_metrics = compute_classification_metric(test_dataset_with_labels,predictions_post_processed,'is_recid',0,1,sex_privileged,sex_unprivileged)\n",
    "compute_fairness_metrics(fairness_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM Accuracy: 0.7106109324758842\n",
      "SVM Recall: 0.04044943820224719\n",
      "SVM F1 Score: 0.07407407407407407\n",
      "SVM Post-processed Accuracy: 0.7086816720257235\n",
      "SVM Post-processed Recall: 0.04044943820224719\n",
      "SVM Post-processed F1 Score: 0.0736196319018405\n"
     ]
    }
   ],
   "source": [
    "svm_classifier = make_pipeline(StandardScaler(), SVC(kernel='linear'))\n",
    "\n",
    "# Addestro il classificatore sui dati di training\n",
    "svm_classifier.fit(X_train, y_train)\n",
    "\n",
    "# After the training phase, the model will be tested by predicting the values on the test set\n",
    "svm_scores = svm_classifier.decision_function(X_test)\n",
    "svm_prediction = svm_classifier.predict(X_test)\n",
    "\n",
    "compute_performance_metrics(y_test,svm_prediction,\"SVM\")\n",
    "\n",
    "# Aggiunta della colonna del target al dataframe di X_test e rimozione di valori mancanti\n",
    "dataset = X_test.copy(deep=True)\n",
    "dataset['is_recid'] = y_test\n",
    "dataset = dataset.dropna()\n",
    "\n",
    "# Conversione dei dati in formato BinaryLabelDataset\n",
    "test_dataset = BinaryLabelDataset(df=dataset, label_names=['is_recid'], protected_attribute_names=sex_features)\n",
    "\n",
    "# Creazione di un dataset di predizioni per il post-processing\n",
    "predictions = dataset.copy(deep=True)\n",
    "predictions['is_recid'] = svm_prediction\n",
    "predictions = predictions.dropna()\n",
    "\n",
    "test_pred_dataset = BinaryLabelDataset(df=predictions, label_names=['is_recid'], protected_attribute_names=sex_features)\n",
    "\n",
    "# Applicazione del post-processing con EqOddsPostprocessing\n",
    "eq_odds = EqOddsPostprocessing(privileged_groups=[{'sex_Female': 1}], unprivileged_groups=[{'sex_Female': 0}])\n",
    "eq_odds = eq_odds.fit(test_dataset, test_pred_dataset)\n",
    "\n",
    "#predizioni post-processate\n",
    "eq_odds_pred = eq_odds.predict(test_pred_dataset)\n",
    "eq_odds_labels = eq_odds_pred.labels\n",
    "\n",
    "# Calcolo delle metriche di performance per il modello post-processato\n",
    "compute_performance_metrics(y_test, eq_odds_labels, \"SVM Post-processed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Statistical Parity Difference (SPD): 0.0\n",
      "Average Odds Difference (AOD): 0.007\n",
      "Equal Opportunity Difference (EOD): -0.001\n"
     ]
    }
   ],
   "source": [
    "# Creazione del dataset di test con le etichette originali e predizioni del modello\n",
    "test_dataset_with_labels = X_test.copy(deep=True)\n",
    "test_dataset_with_labels['is_recid'] = y_test\n",
    "\n",
    "# Creazione del dataset di predizioni del modello post-processato\n",
    "predictions_post_processed = X_test.copy(deep=True)\n",
    "predictions_post_processed['is_recid'] = eq_odds_labels\n",
    "\n",
    "# Calcolo delle metriche di fairness\n",
    "fairness_metrics = compute_classification_metric(test_dataset_with_labels,predictions_post_processed,'is_recid',0,1,sex_privileged,sex_unprivileged)\n",
    "compute_fairness_metrics(fairness_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Accuracy: 0.707395498392283\n",
      "Random Forest Recall: 0.2247191011235955\n",
      "Random Forest F1 Score: 0.3053435114503817\n",
      "Random Forest Post-processed Accuracy: 0.7028938906752411\n",
      "Random Forest Post-processed Recall: 0.11235955056179775\n",
      "Random Forest Post-processed F1 Score: 0.17793594306049823\n"
     ]
    }
   ],
   "source": [
    "rf_classifier = RandomForestClassifier(n_estimators=100, criterion='gini', max_depth = None, random_state=42)\n",
    "\n",
    "rf_classifier.fit(X_train, y_train)\n",
    "\n",
    "rf_predictions = rf_classifier.predict(X_test)\n",
    "\n",
    "compute_performance_metrics(y_test,rf_predictions,\"Random Forest\")\n",
    "\n",
    "# Aggiunta della colonna del target al dataframe di X_test e rimozione di valori mancanti\n",
    "dataset = X_test.copy(deep=True)\n",
    "dataset['is_recid'] = y_test\n",
    "dataset = dataset.dropna()\n",
    "\n",
    "# Conversione dei dati in formato BinaryLabelDataset\n",
    "test_dataset = BinaryLabelDataset(df=dataset, label_names=['is_recid'], protected_attribute_names=sex_features)\n",
    "\n",
    "# Creazione di un dataset di predizioni per il post-processing\n",
    "predictions = dataset.copy(deep=True)\n",
    "predictions['is_recid'] = rf_predictions\n",
    "predictions = predictions.dropna()\n",
    "\n",
    "test_pred_dataset = BinaryLabelDataset(df=predictions, label_names=['is_recid'], protected_attribute_names=sex_features)\n",
    "\n",
    "\n",
    "# Applicazione del post-processing con EqOddsPostprocessing\n",
    "eq_odds = EqOddsPostprocessing(privileged_groups=[{'sex_Female': 1}], unprivileged_groups=[{'sex_Female': 0}])\n",
    "eq_odds = eq_odds.fit(test_dataset, test_pred_dataset)\n",
    "\n",
    "# predizioni post-processate\n",
    "eq_odds_pred = eq_odds.predict(test_pred_dataset)\n",
    "eq_odds_labels = eq_odds_pred.labels\n",
    "\n",
    "# Calcolo delle metriche di performance per il modello post-processato\n",
    "compute_performance_metrics(y_test, eq_odds_labels, \"Random Forest Post-processed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Statistical Parity Difference (SPD): -0.007\n",
      "Average Odds Difference (AOD): 0.002\n",
      "Equal Opportunity Difference (EOD): -0.002\n"
     ]
    }
   ],
   "source": [
    "# Creazione del dataset di test con le etichette originali e predizioni del modello\n",
    "test_dataset_with_labels = X_test.copy(deep=True)\n",
    "test_dataset_with_labels['is_recid'] = y_test\n",
    "\n",
    "# Creazione del dataset di predizioni del modello post-processato\n",
    "predictions_post_processed = X_test.copy(deep=True)\n",
    "predictions_post_processed['is_recid'] = eq_odds_labels\n",
    "\n",
    "# Calcolo delle metriche di fairness\n",
    "fairness_metrics = compute_classification_metric(test_dataset_with_labels,predictions_post_processed,'is_recid',0,1,sex_privileged,sex_unprivileged)\n",
    "compute_fairness_metrics(fairness_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'race_Caucasian': 1, 'race_African-American': 0, 'race_Asian': 0, 'race_Other': 0, 'race_Native American': 0, 'race_Hispanic': 0}]\n",
      "[{'race_African-American': 1, 'race_Caucasian': 0}, {'race_Asian': 1, 'race_Caucasian': 0}, {'race_Other': 1, 'race_Caucasian': 0}, {'race_Native American': 1, 'race_Caucasian': 0}, {'race_Hispanic': 1, 'race_Caucasian': 0}]\n"
     ]
    }
   ],
   "source": [
    "race_privileged_group = [{race_privileged: 1, **{attr: 0 for attr in race_unprivileged}}]\n",
    "race_unprivileged_group = [{attr: 1, race_privileged: 0} for attr in race_unprivileged]\n",
    "\n",
    "print(race_privileged_group)\n",
    "print(race_unprivileged_group)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree Accuracy: 0.6578778135048231\n",
      "Decision Tree Recall: 0.3595505617977528\n",
      "Decision Tree F1 Score: 0.3755868544600939\n",
      "Decision Tree Post-processed Accuracy: 0.6520900321543408\n",
      "Decision Tree Post-processed Recall: 0.29213483146067415\n",
      "Decision Tree Post-processed F1 Score: 0.32459425717852686\n"
     ]
    }
   ],
   "source": [
    "# Train the Decision Tree classifier\n",
    "dt_clf = DecisionTreeClassifier(random_state=42)\n",
    "dt_clf.fit(X_train, y_train)\n",
    "\n",
    "# Predizione sui dati di test\n",
    "dt_predictions = dt_clf.predict(X_test)\n",
    "\n",
    "# Calcolo delle metriche di performance per il Decision Tree grezzo\n",
    "compute_performance_metrics(y_test, dt_predictions, \"Decision Tree\")\n",
    "\n",
    "# Aggiunta della colonna del target al dataframe di X_test e rimozione di valori mancanti\n",
    "dataset = X_test.copy(deep=True)\n",
    "dataset['is_recid'] = y_test\n",
    "dataset = dataset.dropna()\n",
    "\n",
    "# Conversione dei dati in formato BinaryLabelDataset\n",
    "test_dataset = BinaryLabelDataset(df=dataset, \n",
    "                                  label_names=['is_recid'], \n",
    "                                  protected_attribute_names=race_features,\n",
    "                                  favorable_label=0,\n",
    "                                  unfavorable_label=1)\n",
    "\n",
    "# Creazione di un dataset di predizioni per il post-processing\n",
    "predictions = dataset.copy(deep=True)\n",
    "predictions['is_recid'] = dt_predictions\n",
    "predictions = predictions.dropna()\n",
    "\n",
    "test_pred_dataset = BinaryLabelDataset(df=predictions, \n",
    "                                       label_names=['is_recid'], \n",
    "                                       protected_attribute_names=race_features,\n",
    "                                       favorable_label=0,\n",
    "                                       unfavorable_label=1,)\n",
    "\n",
    "# Applicazione del post-processing con EqOddsPostprocessing\n",
    "eq_odds = EqOddsPostprocessing(privileged_groups=race_privileged_group, unprivileged_groups=race_unprivileged_group)\n",
    "eq_odds = eq_odds.fit(test_dataset, test_pred_dataset)\n",
    "\n",
    "# predizioni post-processate\n",
    "eq_odds_pred = eq_odds.predict(test_pred_dataset)\n",
    "eq_odds_labels = eq_odds_pred.labels\n",
    "\n",
    "# Calcolo delle metriche di performance per il modello post-processato\n",
    "compute_performance_metrics(y_test, eq_odds_labels, \"Decision Tree Post-processed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Statistical Parity Difference (SPD): -0.009\n",
      "Average Odds Difference (AOD): 0.004\n",
      "Equal Opportunity Difference (EOD): -0.009\n"
     ]
    }
   ],
   "source": [
    "# Creazione del dataset di test con le etichette originali e predizioni del modello\n",
    "test_dataset_with_labels = X_test.copy(deep=True)\n",
    "test_dataset_with_labels['is_recid'] = y_test\n",
    "\n",
    "# Creazione del dataset di predizioni del modello post-processato\n",
    "predictions_post_processed = X_test.copy(deep=True)\n",
    "predictions_post_processed['is_recid'] = eq_odds_labels\n",
    "\n",
    "# Calcolo delle metriche di fairness\n",
    "fairness_metrics = compute_classification_metric(test_dataset_with_labels,predictions_post_processed,'is_recid',0,1,race_privileged,race_unprivileged)\n",
    "compute_fairness_metrics(fairness_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM Accuracy: 0.7106109324758842\n",
      "SVM Recall: 0.04044943820224719\n",
      "SVM F1 Score: 0.07407407407407407\n",
      "SVM Post-processed Accuracy: 0.7086816720257235\n",
      "SVM Post-processed Recall: 0.033707865168539325\n",
      "SVM Post-processed F1 Score: 0.062111801242236024\n"
     ]
    }
   ],
   "source": [
    "svm_classifier = make_pipeline(StandardScaler(), SVC(kernel='linear'))\n",
    "\n",
    "# Addestro il classificatore sui dati di training\n",
    "svm_classifier.fit(X_train, y_train)\n",
    "\n",
    "# After the training phase, the model will be tested by predicting the values on the test set\n",
    "svm_scores = svm_classifier.decision_function(X_test)\n",
    "svm_prediction = svm_classifier.predict(X_test)\n",
    "\n",
    "compute_performance_metrics(y_test,svm_prediction,\"SVM\")\n",
    "\n",
    "# Aggiunta della colonna del target al dataframe di X_test e rimozione di valori mancanti\n",
    "dataset = X_test.copy(deep=True)\n",
    "dataset['is_recid'] = y_test\n",
    "dataset = dataset.dropna()\n",
    "\n",
    "# Conversione dei dati in formato BinaryLabelDataset\n",
    "test_dataset = BinaryLabelDataset(df=dataset, label_names=['is_recid'], protected_attribute_names=race_features)\n",
    "\n",
    "# Creazione di un dataset di predizioni per il post-processing\n",
    "predictions = dataset.copy(deep=True)\n",
    "predictions['is_recid'] = svm_prediction\n",
    "predictions = predictions.dropna()\n",
    "\n",
    "test_pred_dataset = BinaryLabelDataset(df=predictions, label_names=['is_recid'], protected_attribute_names=race_features)\n",
    "\n",
    "# Applicazione del post-processing con EqOddsPostprocessing\n",
    "eq_odds = EqOddsPostprocessing(privileged_groups=race_privileged_group, unprivileged_groups=race_unprivileged_group)\n",
    "eq_odds = eq_odds.fit(test_dataset, test_pred_dataset)\n",
    "\n",
    "# predizioni post-processate\n",
    "eq_odds_pred = eq_odds.predict(test_pred_dataset)\n",
    "eq_odds_labels = eq_odds_pred.labels\n",
    "\n",
    "# Calcolo delle metriche di performance per il modello post-processato\n",
    "compute_performance_metrics(y_test, eq_odds_labels, \"SVM Post-processed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Statistical Parity Difference (SPD): -0.001\n",
      "Average Odds Difference (AOD): 0.001\n",
      "Equal Opportunity Difference (EOD): -0.001\n"
     ]
    }
   ],
   "source": [
    "# Creazione del dataset di test con le etichette originali e predizioni del modello\n",
    "test_dataset_with_labels = X_test.copy(deep=True)\n",
    "test_dataset_with_labels['is_recid'] = y_test\n",
    "\n",
    "# Creazione del dataset di predizioni del modello post-processato\n",
    "predictions_post_processed = X_test.copy(deep=True)\n",
    "predictions_post_processed['is_recid'] = eq_odds_labels\n",
    "\n",
    "# Calcolo delle metriche di fairness\n",
    "fairness_metrics = compute_classification_metric(test_dataset_with_labels,predictions_post_processed,'is_recid',0,1,race_privileged,race_unprivileged)\n",
    "compute_fairness_metrics(fairness_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Accuracy: 0.707395498392283\n",
      "Random Forest Recall: 0.2247191011235955\n",
      "Random Forest F1 Score: 0.3053435114503817\n",
      "Random Forest Post-processed Accuracy: 0.704823151125402\n",
      "Random Forest Post-processed Recall: 0.14831460674157304\n",
      "Random Forest Post-processed F1 Score: 0.2233502538071066\n"
     ]
    }
   ],
   "source": [
    "rf_classifier = RandomForestClassifier(n_estimators=100, criterion='gini', max_depth = None, random_state=42)\n",
    "\n",
    "rf_classifier.fit(X_train, y_train)\n",
    "\n",
    "rf_predictions = rf_classifier.predict(X_test)\n",
    "\n",
    "compute_performance_metrics(y_test,rf_predictions,\"Random Forest\")\n",
    "\n",
    "# Aggiunta della colonna del target al dataframe di X_test e rimozione di valori mancanti\n",
    "dataset = X_test.copy(deep=True)\n",
    "dataset['is_recid'] = y_test\n",
    "dataset = dataset.dropna()\n",
    "\n",
    "# Conversione dei dati in formato BinaryLabelDataset\n",
    "test_dataset = BinaryLabelDataset(df=dataset, label_names=['is_recid'], protected_attribute_names=race_features)\n",
    "\n",
    "# Creazione di un dataset di predizioni per il post-processing\n",
    "predictions = dataset.copy(deep=True)\n",
    "predictions['is_recid'] = rf_predictions\n",
    "predictions = predictions.dropna()\n",
    "\n",
    "test_pred_dataset = BinaryLabelDataset(df=predictions, label_names=['is_recid'], protected_attribute_names=race_features)\n",
    "\n",
    "\n",
    "# Applicazione del post-processing con EqOddsPostprocessing\n",
    "eq_odds = EqOddsPostprocessing(privileged_groups=race_privileged_group, unprivileged_groups=race_unprivileged_group)\n",
    "eq_odds = eq_odds.fit(test_dataset, test_pred_dataset)\n",
    "\n",
    "# predizioni post-processate\n",
    "eq_odds_pred = eq_odds.predict(test_pred_dataset)\n",
    "eq_odds_labels = eq_odds_pred.labels\n",
    "\n",
    "# Calcolo delle metriche di performance per il modello post-processato\n",
    "compute_performance_metrics(y_test, eq_odds_labels, \"Random Forest Post-processed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Statistical Parity Difference (SPD): -0.006\n",
      "Average Odds Difference (AOD): -0.001\n",
      "Equal Opportunity Difference (EOD): 0.002\n"
     ]
    }
   ],
   "source": [
    "# Creazione del dataset di test con le etichette originali e predizioni del modello\n",
    "test_dataset_with_labels = X_test.copy(deep=True)\n",
    "test_dataset_with_labels['is_recid'] = y_test\n",
    "\n",
    "# Creazione del dataset di predizioni del modello post-processato\n",
    "predictions_post_processed = X_test.copy(deep=True)\n",
    "predictions_post_processed['is_recid'] = eq_odds_labels\n",
    "\n",
    "# Calcolo delle metriche di fairness\n",
    "fairness_metrics = compute_classification_metric(test_dataset_with_labels,predictions_post_processed,'is_recid',0,1,race_privileged,race_unprivileged)\n",
    "compute_fairness_metrics(fairness_metrics)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
